{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3464e0-ea06-4c0f-a4d6-adbf45c9a486",
   "metadata": {},
   "source": [
    "# Intro\n",
    "* familiar bit: few shot, chain of reasoning style prompt\n",
    "* works with off the shelf models, no fine tuning, you can use this method with LLMs you get over api calls.\n",
    "* TODO: screenshot of cot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49588ff-d498-44c1-a793-d33ec751245f",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "* screen shot of algo demo from paper\n",
    "* picture of Jon's diagram\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759d09a-1a5a-44bc-8d9d-beadfe02864b",
   "metadata": {},
   "source": [
    "# code\n",
    "babbage results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f13d022-a928-4d20-8863-78e5c4c42830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd05cce5-0e5a-4773-b42f-062b0fa2a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import Completion\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a60f7004-3c23-429c-ac27-3c4b9eb8c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key_path = '/root/.openai/key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15b033b6-c314-4cc9-aa72-1e0135c4f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt='''Please answer the given question by writing out the chain of reasoning, and then saying the answer. End your answer in the form 'The answer is <answer>.'\n",
    "\n",
    "Examples:\n",
    "\n",
    "Q: John has 2 eggs. He gets 3 more from the store. How many eggs does he end up with?\n",
    "A: Starting with 2 he gets 3 more. 2 + 3 = 5. The answer is 5.\n",
    "\n",
    "Q: Bill makes $12 an hour. He works for 4 hours Monday and 5 hours Tuesday. How much money does he make these two days?\n",
    "A: Monday he made 4 * $12 = $48, and Tuesday he made 5 * $12 = $60. In total he made $48 + $60 = $108. The answer is $108.\n",
    "\n",
    "Q: There are 4 birds in a tree. 3 more fly in to join them. How many birds are there total?\n",
    "A: First there are 4, and 3 more makes 4 + 3 = 7. The answer is 7.\n",
    "\n",
    "Q: Last year Dale bought a book for $18.30, which was 25% off the sticker price. What was the sticker price?\n",
    "A: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de2dc0-d79d-4710-b6c4-5b8e7818755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single = Completion.create(prompt=few_shot_prompt,\n",
    "                      model=\"davinci-002\", max_tokens=75, n=1, temperature=0.0)\n",
    "single['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9863233-8580-40d3-a58f-99bb07851dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = Completion.create(prompt=few_shot_prompt,\n",
    "                      model=\"davinci-002\", temperature=.5,  max_tokens=75, n=50, stop=\"Q: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "864cd237-c5c9-4e6f-80bd-901f81adf222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n",
      "100% of the sticker price is $18.30. 100% / 25% = 4. The answer is $18.30 * 4 = $73.20.\n",
      "\n",
      "\n",
      "=====\n",
      "25% of $18.30 is $4.57. The sticker price is $18.30 + $4.57 = $22.87. The answer is $22.87.\n",
      "\n",
      "\n",
      "=====\n",
      "18.30 / 0.75 = 24.4, so the sticker price was $24.40.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response in completions['choices'][:3]:\n",
    "    print('=====\\n' + response['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e728174-3ee3-47eb-ada5-5c5795dd8029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$18.30',\n",
       " '$22.87.',\n",
       " '$24.40.',\n",
       " '$13.73.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$32.05.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$22.875.',\n",
       " '22.875.',\n",
       " '$13.725.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$22.87.',\n",
       " '$22.88.',\n",
       " '$4.57.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$24.40.',\n",
       " '$13.73.',\n",
       " '$24.40.',\n",
       " '$13.73.',\n",
       " '$22.88.',\n",
       " '$23.12.',\n",
       " '$22.87.',\n",
       " '$24.40.']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'answer is ([^\\s]+)')\n",
    "answers = []\n",
    "for response in completions['choices']:\n",
    "    matches = pattern.findall(response['text'])\n",
    "    if len(matches) > 0:\n",
    "        answers.append(matches[0])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "905e4f49-fbb8-4f7a-a2c6-7362d8c265cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$24.40.', 16),\n",
       " ('$22.87.', 3),\n",
       " ('$13.73.', 3),\n",
       " ('$22.88.', 2),\n",
       " ('$18.30', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter(answers)\n",
    "ctr.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5e55b-0af9-4a9e-b9b8-ac04ae2c6775",
   "metadata": {},
   "source": [
    "# Results\n",
    "## parameters\n",
    "40 samples, temperature sampling t=.5, topk=40, but results compatible with many sampling strategies\n",
    "## specific result improvement example\n",
    "davinci-002 / GSM8k, 60.1 -> 78, + 17.9% difference\n",
    "screenshot of the gsm8k question\n",
    "\n",
    "## Screenshot of the greedy vs. sampled paths for palm-540b\n",
    "screenshot of row example from appendix, palm also saw 17.9% improvement\n",
    "\n",
    "## table of results\n",
    "lots of info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c6c06-a054-4324-bcf7-3140dcd3a821",
   "metadata": {},
   "source": [
    "# Compare to other approaches \n",
    "* multimodel ensemble - only 3, unsure exactly how algo worked for this, but did worse\n",
    "* ensemble of prompts - confusing\n",
    "* beam search (used by itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a79e90-9f21-4bac-8799-1c0096710f41",
   "metadata": {},
   "source": [
    "# Effects of parameters\n",
    "* num reasoning paths\n",
    "  * perf saturation after 10\n",
    "* temperature, top k, top p\n",
    "* sampling method - beam search works, but has less diversity than other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb94a2-c0a0-440e-b08f-b8093b7b2c6c",
   "metadata": {},
   "source": [
    "# Observations\n",
    "* bigger effect with higher model size\n",
    "* consistency % is confidence indicator\n",
    "  * show graph\n",
    "* trade 1 time compute at train for permanent 10x or more compute cost increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
